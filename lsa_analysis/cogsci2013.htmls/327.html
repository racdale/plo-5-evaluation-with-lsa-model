<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<HTML>

<HEAD>
  <LINK rel=stylesheet HREF="../../style/style.css">
  <TITLE></TITLE>
</HEAD>

<BODY>

  <div id=pageheader>
    <div id=logo><a href="http://cognitivesciencesociety.org/conference2013/index.html"><img src="../../style/logo.png" title="CogSci 2013 logo" alt="CogSci 2013 logo"></a></div>
    <div id=title></div>
    <br clear=both>
    <div id=menubar>
      <ul>
	<li><a href="../../index.html">Table of Contents</a>
      </ul>
    </div>
  </div>

  <div id=pagebody>

    

	<h1>Probabilistic Language Modeling with Hidden Stochastic Automata</h1>

    
	  <ul>
    	    <li>Mark Andrews, <em>Nottingham Trent University and University College London</em>
                                        	  </ul>

    

	<h2>Abstract</h2>

          <p id=abstract>In this paper, we introduce a novel dynamical Bayesian network
      model for probabilistic language modeling. We refer to this as the Hidden
      Stochastic Automaton. This model, while based on a generalization of the Hidden
      Markov model, has qualitatively greater generative power than either the Hidden
      Markov model itself or any of its existing variants and generalizations. This
      allows the Hidden Stochastic Automaton to be used as a probabilistic model of
      natural languages in a way that is not possible with existing dynamical Bayesian
      networks. Its relevance to Cognitive Science is primarily as a computational ---
      in the Marr (1982) sense of the term --- model of cognition, but potentially also
      as a model of resource bounded cognitive processing, and as a model of the
      implementation of computation in physical dynamical systems.</p>

    





	  <ul>


	    <li id=files>The Paper: <a href="paper0327.pdf">Probabilistic Language Modeling with Hidden Stochastic Automata</a>


	  </ul>

	<p><br><a href="../../index.html">Back to Table of Contents</a>

  </div>

  <div id=pagefooter>
  </div>

</BODY>
</HTML>

