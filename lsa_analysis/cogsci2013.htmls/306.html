<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<HTML>

<HEAD>
  <LINK rel=stylesheet HREF="../../style/style.css">
  <TITLE></TITLE>
</HEAD>

<BODY>

  <div id=pageheader>
    <div id=logo><a href="http://cognitivesciencesociety.org/conference2013/index.html"><img src="../../style/logo.png" title="CogSci 2013 logo" alt="CogSci 2013 logo"></a></div>
    <div id=title></div>
    <br clear=both>
    <div id=menubar>
      <ul>
	<li><a href="../../index.html">Table of Contents</a>
      </ul>
    </div>
  </div>

  <div id=pagebody>

    

	<h1>Cheap but Clever: Human Active Learning in a Bandit Setting</h1>

    
	  <ul>
    	    <li>Shunan Zhang, <em>University of California, San Diego, La Jolla, California, USA</em>
    	    <li>Angela Yu, <em>University of California San Diego, La Jolla, CA, United States</em>
                                    	  </ul>

    

	<h2>Abstract</h2>

          <p id=abstract>How people achieve long-term goals in an imperfectly known
      environment, via repeated tries and noisy outcomes, is an important problem in
      cognitive science. There are two interrelated questions: how humans represent
      information, both what has been learned and what can still be learned, and how
      they choose actions, in particular how they negotiate the tension between
      exploration and exploitation. In this work, we examine human behavioral data in a
      multi-armed bandit setting, in which the subject choose one of four
      &#8220;arms&#8221; to pull on each trial and receives a binary outcome
      (win/lose). We implement both the Bayes optimal policy, which maximizes the
      expected cumulative reward in this &#64257;nite horizon bandit environment, as
      well as a variety of heuristic policies that vary in their complexity of
      information representation and decision policy. We &#64257;nd that the knowledge
      gradient algorithm, which combines exact Bayesian learning with a decision policy
      that maximizes a combination of immediate reward gain and long-term knowledge
      gain, captures subjects&#8217; trial-by-trial choice best among all the models
      considered; it also provides the best approximation to the computationally
      intense optimal policy among all the heuristic policies.</p>

    





	  <ul>


	    <li id=files>The Paper: <a href="paper0306.pdf">Cheap but Clever: Human Active Learning in a Bandit Setting</a>


	  </ul>

	<p><br><a href="../../index.html">Back to Table of Contents</a>

  </div>

  <div id=pagefooter>
  </div>

</BODY>
</HTML>

